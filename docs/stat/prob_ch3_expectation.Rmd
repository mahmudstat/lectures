---
title: "Probability (Expectation)"
author: Abdullah Al Mahmud
date: \today
output: 
  revealjs::revealjs_presentation:
    theme: sky
    transition: zoom
    reveal_plugins: ["chalkboard", "zoom", "menu"]
    reveal_options:
      chalkboard:
        theme: whiteboard
    self_contained: false
    lib_dir: libs
    css: css/styles.css
    includes:
      in_header: css/header.html
---

# Expectation vs AVerage (AM)

|  Accident | Frequency<br>(f)  | Relative<br>Frequency<br>(rf) |
|:---------:|:-----------------:|:-----------------------------:|
|    Bus    |         70        |              0.56             |
|   Train   |         35        |              0.28             |
|    Ship   |         20        |              0.16             |
| **Total** |      **125**      |             **1**             |

> - Do rf's look like probabilities?
> - AM = ?

## Weighted Mean Revisited

| Daily<br>Income<br>($x$) | Frequency<br>($f$) | Relative<br>Frequency<br>($rf$) | $x \times f$ | $x \times rf$ |
|:------------------------:|:------------------:|:-------------------------------:|:------------:|:-------------:|
|            400           |         70         |               0.56              |     28000    |      224      |
|            500           |         35         |               0.28              |     17500    |      140      |
|            800           |         20         |               0.16              |     16000    |      128      |
|         **Total**        |       **125**      |              **1**              |     61500    |      492      |

If a man is randomly selected, what is the probability that his daily income is 400 BDT?

> - In expectation, $rf$ is the weight

## Expectation from Mean

$\displaystyle \bar X=\frac{\Sigma x_if_i}{\Sigma f_i}$ 

> - $= \frac{x_1f_1+x_2f_2+\cdots+x_3f_3}{N}$ (letting $\Sigma f_i=N$)
> - $= x_1\frac{f_1}{N}+x_2\frac{f_2}{N}+\cdots +x_2\frac{f_n}{N}$
> - $= x_1p_1+x_2p_2+\cdots+x_np_n$
> - $= \sum x_i\cdot p(x_i)$

## Expectation

Probabilities are weights

\begin{eqnarray} 
E(X) &=& x_1 \times P(x_1) + x_2 \times P(x_2)+ \cdots +x_n \times P(x_n)      \nonumber \\
&=& \displaystyle \sum_{i=1}^n x_i \times P(x_i) \nonumber \\
\end{eqnarray}

<div onclick="klikaj('expcont')"><span>For continuous X</span></div>
<div id="expcont" style="visibility: hidden">
$\displaystyle E(X) = \int_{-\infty}^{\infty} f(x)dx$
</div>
<script>
function klikaj(i) {
    document.getElementById(i).style.visibility='visible';
}
</script>

> - $E(X^2) = \sum x^2 \times p(x)$

## Expectation Example

|   $X$  |      0      |      1      |      2      |
|:------:|:-----------:|:-----------:|:-----------:|
| $P(x)$ | $\frac 1 4$ | $\frac 1 2$ | $\frac 1 4$ |

Find 

i) $E(X)$
ii) $E(X^2)$

## Properties of Expectation

<a href="ch3_xi_stat.html#/properties-of-am" target="_blank">Aslo refer to the properties of AM</a> ($E(X)=\bar X$)

> - Expectation of a constant, $E(c)=c$
> - $E(aX)=aE(X)$
> - $E(X-a) = E(X) - E(a)$
> - $E(aX+b) = aE(X)+b$
> - $E(X+Y) = E(X)+E(Y)$
> - $E(XY) = E(X) E(Y)$ (if independent, relate with probability)
> - $E(X^2)\ge \{E(X)\}^2$
> - $E\left(\frac 1 X \right) \ge \frac 1 {E(X)}$

## Variance

<a href="ch4_xi_stat_dispersion.html#/variance-and-sd" target="_blank">Recall this (click)</a>

We knew, $\sigma^2 = \bar {X^2} - (\bar X)^2=$ Mean of square - square of mean

> - $V(X)=E(X^2)-\{E(X)\}^2$ 
> - Original Formula: $V(X) = E\{x-E(X)\}^2$ (Match)
> - Expand and prove these are equal

## Variance Properties

> - $V(c)=0$ ponder, why?
> - $V(X+a)=V(X)$
> - $V(aX+c) = a^2V(X)$ (depends only on scale, recall)
> - $V(X+Y) = V(X)+V(Y)$

## Covariance

$Cov(X,Y) = \frac{\Sigma (x-\bar x)(y-\bar x)}{n}$

Write in terms of $E(X)$

> - $Cov(X,Y) = E [\{x-E(X)\}\{y-E(Y)\}]$ (Expand)
> - $E(XY)-E(X)E(Y)$ 

## Prove E(X) Properties

[Go back to see the properties](#/properties-of-expectation)

## E(a)

$E(a) = \sum a \cdot p(x_i)$

> - $=a \cdot \sum p(x_i)$
> - $=a \times 1 = a$
> - Others can be proven similarly

## Double Summation Revisited

| Exam (X) $\to$<br>Result (Y) $\downarrow$ |   PSC  |   JSC  |   SSC  |   HSC  |  Total  |
|:-----------------------------------------:|:------:|:------:|:------:|:------:|:-------:|
|                   Passed                  |   30   |   26   |   23   |   25   | **104** |
|                   Failed                  |   12   |   13   |   10   |   14   |  **49** |
|                   Absent                  |    5   |    2   |    3   |    4   |  **14** |
|                 **Total**                 | **47** | **41** | **36** | **43** | **167** |

> - If sum over $x_i$, we get only 1 column at a time
> - If sum over $y_i$, we get only 1 row at a time
> - SO to get grand total, we must use $\sum \sum (x_i+y_j)$

## E(X+Y) & E(XY)

<small>

$E(X+Y)$ 

> - $= \displaystyle \sum_{i=1}^m \sum_{j=1}^n (x_i+y_j)P(x_i,y_j)$
> - $=\displaystyle \sum_{i=1}^m \sum_{j=1}^n \{x_iP(x_i,y_j)+y_jP(x_i,y_j)\}$
> - $=\displaystyle \sum_{i=1}^m \sum_{j=1}^n x_iP(x_i,y_j)+\sum_{i=1}^m \sum_{j=1}^n y_jP(x_i,y_j)$
> - $=\displaystyle \sum_{i=1}^m x_i\sum_{j=1}^n P(x_i,y_j)+\sum_{j=1}^n x_i\sum_{i=1}^m P(x_i,y_j)$
> - $=\displaystyle \sum_{i=1}^m x_i P(x_i) + \sum_{j=1}^n y_j P(y_j)$
> - $=E(X)+E(Y)$

</small>

## E(X^2^) â‰¥ {E(X)}^2^

$V(X)\ge 0$

> - $\therefore E(X^2) - \{E(X)\}^2 \ge 0$

## AM & HM

$E(\frac 1 X) \ge \frac 1 {E(X)}$

> - $AM \ge HM$
> - $HM =$ opposite of mean of opposite $\to \frac{1}{\frac{\sum x_i}{n}}$
> - $E(X) \ge \frac{1}{E(\frac 1 x)}$
> - If $0.5 \gt \frac 1 3 \to 3 \gt  \frac 1 {0.5}$

## Demo Slide

Contents

